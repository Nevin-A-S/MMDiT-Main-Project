{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zyro/miniconda3/envs/py310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from diffusers.models import AutoencoderKL\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.datasets import CIFAR10\n",
    "from tqdm import tqdm\n",
    "\n",
    "from opendit.diffusion import create_diffusion\n",
    "from opendit.models.mmdit import MMDiT_models\n",
    "from opendit.utils.data_utils import get_transforms_image\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, csv_path, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_path (string): Path to the CSV file with annotations\n",
    "            root_dir (string): Base directory for image paths in CSV\n",
    "            transform (callable, optional): Optional transform to be applied on images\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        img_rel_path = Path(row['File Path'].replace(\"\\\\\", \"/\")) \n",
    "        img_full_path = self.root_dir / img_rel_path\n",
    "\n",
    "        try:\n",
    "            if not img_full_path.exists():\n",
    "                raise FileNotFoundError(f\"Image not found at: {img_full_path}\")\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(FileNotFoundError)\n",
    "        image = Image.open(img_full_path).convert('RGB')\n",
    "        \n",
    "        caption = row['Caption']\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_crop_arr(pil_image, image_size):\n",
    "\n",
    "\n",
    "    while min(*pil_image.size) >= 2 * image_size:\n",
    "        pil_image = pil_image.resize(\n",
    "            tuple(x // 2 for x in pil_image.size), resample=Image.BOX\n",
    "        )\n",
    "\n",
    "    scale = image_size / min(*pil_image.size)\n",
    "    pil_image = pil_image.resize(\n",
    "        tuple(round(x * scale) for x in pil_image.size), resample=Image.BICUBIC\n",
    "    )\n",
    "\n",
    "    arr = np.array(pil_image)\n",
    "    crop_y = (arr.shape[0] - image_size) // 2\n",
    "    crop_x = (arr.shape[1] - image_size) // 2\n",
    "    return Image.fromarray(arr[crop_y: crop_y + image_size, crop_x: crop_x + image_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def requires_grad(model, flag=True):\n",
    "    \"\"\"Enable/disable gradients for a model's parameters.\"\"\"\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_ema(ema, model, decay=0.9999):\n",
    "    \"\"\"Update EMA parameters.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        for ema_param, model_param in zip(ema.parameters(), model.parameters()):\n",
    "            ema_param.data.mul_(decay).add_(model_param.data, alpha=1 - decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"Trains a new MMDiT model.\"\"\"\n",
    "assert torch.cuda.is_available(), \"Training currently requires at least one GPU.\"\n",
    "\n",
    "# Setup directories\n",
    "os.makedirs(args.outputs, exist_ok=True)\n",
    "experiment_index = len(glob(f\"{args.outputs}/*\"))\n",
    "model_string_name = args.model.replace(\"/\", \"-\")\n",
    "experiment_dir = f\"{args.outputs}/{experiment_index:03d}-{model_string_name}\"\n",
    "os.makedirs(experiment_dir, exist_ok=True)\n",
    "\n",
    "# Save configuration\n",
    "with open(f\"{experiment_dir}/config.txt\", \"w\") as f:\n",
    "    json.dump(args.__dict__, f, indent=4)\n",
    "\n",
    "# Setup tensorboard\n",
    "tensorboard_dir = f\"{experiment_dir}/tensorboard\"\n",
    "os.makedirs(tensorboard_dir, exist_ok=True)\n",
    "writer = SummaryWriter(tensorboard_dir)\n",
    "\n",
    "# Setup device and dtype\n",
    "device = torch.device('cuda')\n",
    "if args.mixed_precision == \"bf16\":\n",
    "    dtype = torch.bfloat16\n",
    "elif args.mixed_precision == \"fp16\":\n",
    "    dtype = torch.float16\n",
    "else:\n",
    "    dtype = torch.float32\n",
    "\n",
    "# Create VAE encoder\n",
    "vae = AutoencoderKL.from_pretrained(f\"stabilityai/sd-vae-ft-{args.vae}\").to(device)\n",
    "\n",
    "# Configure input size\n",
    "assert args.image_size % 8 == 0, \"Image size must be divisible by 8 (for the VAE encoder).\"\n",
    "input_size = args.image_size // 8\n",
    "\n",
    "# Create model\n",
    "model_config = {\n",
    "    \"input_size\": input_size,\n",
    "    \"num_classes\": args.num_classes,\n",
    "    \"clip_text_encoder\": args.text_encoder,\n",
    "    \"t5_text_encoder\": args.t5_text_encoder,\n",
    "}\n",
    "\n",
    "# Initialize model\n",
    "model_class = MMDiT_models[args.model]\n",
    "model = model_class(**model_config).to(device, dtype=dtype)\n",
    "\n",
    "print(f\"Model has {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "\n",
    "\n",
    "if args.grad_checkpoint:\n",
    "    model.enable_gradient_checkpointing()\n",
    "\n",
    "# Create EMA model\n",
    "ema = MMDiT_models[args.model](**model_config).to(device)\n",
    "ema.load_state_dict(model.state_dict())\n",
    "requires_grad(ema, False)\n",
    "\n",
    "# Create diffusion\n",
    "diffusion = create_diffusion(timestep_respacing=\"\")\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=args.lr,\n",
    "    weight_decay=0\n",
    ")\n",
    "# Setup dataset\n",
    "# dataset = CIFAR10(\n",
    "#     args.data_path,\n",
    "#     transform=get_transforms_image(args.image_size),\n",
    "#     download=True\n",
    "# )\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda pil_image: center_crop_arr(pil_image, args.image_size)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], inplace=True),\n",
    "])\n",
    "\n",
    "csv_path = \"datasets/anime/image_labels.csv\"\n",
    "root_dir = \"datasets/anime\"  \n",
    "\n",
    "dataset = ImageCaptionDataset(\n",
    "csv_path=csv_path,\n",
    "root_dir=root_dir,\n",
    "transform=transform\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    "\n",
    ")\n",
    "\n",
    "print(f\"Dataset contains {len(dataset):,} images ({args.data_path})\")\n",
    "\n",
    "# Ensure EMA is initialized with synced weights\n",
    "update_ema(ema, model, decay=0)\n",
    "model.train()\n",
    "ema.eval()\n",
    "\n",
    "print(f\"Training for {args.epochs} epochs...\")\n",
    "num_steps_per_epoch = len(dataloader)\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    print(f\"Beginning epoch {epoch}...\")\n",
    "    \n",
    "    with tqdm(range(num_steps_per_epoch), desc=f\"Epoch {epoch}\") as pbar:\n",
    "        for step in pbar:\n",
    "            # Get batch\n",
    "            x, y = next(iter(dataloader))\n",
    "            x = x.to(device)\n",
    "\n",
    "            # VAE encode\n",
    "            with torch.no_grad():\n",
    "                x = vae.encode(x).latent_dist.sample().mul_(0.18215)\n",
    "\n",
    "            # print('vae encode:', x.shape)\n",
    "\n",
    "            # Diffusion training step\n",
    "            t = torch.randint(0, diffusion.num_timesteps, (x.shape[0],), device=device)\n",
    "            model_kwargs = dict(c=y)\n",
    "            loss_dict = diffusion.training_losses(model, x, t, model_kwargs)\n",
    "            loss = loss_dict[\"loss\"].mean()\n",
    "            \n",
    "            # Optimization step\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update EMA\n",
    "            update_ema(ema, model)\n",
    "\n",
    "            # Logging\n",
    "            global_step = epoch * num_steps_per_epoch + step\n",
    "            pbar.set_postfix({\"loss\": loss.item(), \"step\": step, \"global_step\": global_step})\n",
    "\n",
    "            if (global_step + 1) % args.log_every == 0:\n",
    "                writer.add_scalar(\"loss\", loss.item(), global_step)\n",
    "\n",
    "            # Save checkpoint\n",
    "            if args.ckpt_every > 0 and (global_step + 1) % args.ckpt_every == 0:\n",
    "                checkpoint = {\n",
    "                    'model': model.state_dict(),\n",
    "                    'ema': ema.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                    'global_step': global_step,\n",
    "                }\n",
    "                torch.save(\n",
    "                    checkpoint,\n",
    "                    f\"{experiment_dir}/checkpoint_{global_step:07d}.pt\"\n",
    "                )\n",
    "                print(f\"Saved checkpoint at global step {global_step}\")\n",
    "\n",
    "print(\"Training finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--model {MMDiT-XL/2,MMDiT-L/4}]\n",
      "                             [--vae {ema,mse}] [--outputs OUTPUTS]\n",
      "                             [--data_path DATA_PATH] [--image_size {256,512}]\n",
      "                             [--num_classes NUM_CLASSES] [--epochs EPOCHS]\n",
      "                             [--batch_size BATCH_SIZE]\n",
      "                             [--num_workers NUM_WORKERS]\n",
      "                             [--log_every LOG_EVERY] [--ckpt_every CKPT_EVERY]\n",
      "                             [--mixed_precision {bf16,fp16,fp32}] [--lr LR]\n",
      "                             [--grad_checkpoint] [--text_encoder TEXT_ENCODER]\n",
      "                             [--t5_text_encoder T5_TEXT_ENCODER]\n",
      "                             [--chkptnumber CHKPTNUMBER]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=/home/zyro/.local/share/jupyter/runtime/kernel-v3ec2d9e305a0ccd0b3fe1f6b84ec5ba37bd8a8e05.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zyro/miniconda3/envs/py310/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.parser = argparse.ArgumentParser(description=\"Training configuration for MMDiT model\")\n",
    "        self._add_arguments()\n",
    "\n",
    "    def _add_arguments(self):\n",
    "        \"\"\"Add all configuration arguments to the parser.\"\"\"\n",
    "        self.parser.add_argument(\"--model\", type=str, choices=[\"MMDiT-XL/2\", \"MMDiT-L/4\"], default=\"MMDiT-S/8\",\n",
    "                                help=\"Model architecture to use\")\n",
    "        self.parser.add_argument(\"--vae\", type=str, choices=[\"ema\", \"mse\"], default=\"ema\",\n",
    "                                help=\"VAE type to use for encoding\")\n",
    "        self.parser.add_argument(\"--outputs\", type=str, default=\"./outputs\",\n",
    "                                help=\"Directory to save outputs (checkpoints, logs, etc.)\")\n",
    "        self.parser.add_argument(\"--data_path\", type=str, default=\"./datasets\",\n",
    "                                help=\"Path to the dataset directory\")\n",
    "        self.parser.add_argument(\"--image_size\", type=int, choices=[256, 512], default=256,\n",
    "                                help=\"Size of input images (must be divisible by 8)\")\n",
    "        self.parser.add_argument(\"--num_classes\", type=int, default=1000,\n",
    "                                help=\"Number of classes for classification (if applicable)\")\n",
    "        self.parser.add_argument(\"--epochs\", type=int, default=1400,\n",
    "                                help=\"Number of training epochs\")\n",
    "        self.parser.add_argument(\"--batch_size\", type=int, default=32,\n",
    "                                help=\"Batch size for training\")\n",
    "        self.parser.add_argument(\"--num_workers\", type=int, default=4,\n",
    "                                help=\"Number of workers for data loading\")\n",
    "        self.parser.add_argument(\"--log_every\", type=int, default=10,\n",
    "                                help=\"Log training metrics every N steps\")\n",
    "        self.parser.add_argument(\"--ckpt_every\", type=int, default=1000,\n",
    "                                help=\"Save a checkpoint every N steps\")\n",
    "        self.parser.add_argument(\"--mixed_precision\", type=str, default=\"bf16\", choices=[\"bf16\", \"fp16\", \"fp32\"],\n",
    "                                help=\"Mixed precision training mode\")\n",
    "        self.parser.add_argument(\"--lr\", type=float, default=1e-4,\n",
    "                                help=\"Learning rate for the optimizer\")\n",
    "        self.parser.add_argument(\"--grad_checkpoint\", action=\"store_true\",\n",
    "                                help=\"Enable gradient checkpointing to save memory\")\n",
    "        self.parser.add_argument(\"--text_encoder\", type=str, default=\"openai/clip-vit-base-patch32\",\n",
    "                                help=\"Text encoder model for CLIP\")\n",
    "        self.parser.add_argument(\"--t5_text_encoder\", type=str, default=\"google-t5/t5-small\",\n",
    "                                help=\"Text encoder model for T5\")\n",
    "        self.parser.add_argument(\"--chkptnumber\", type=str, default=\"None\",\n",
    "                                help=\"Checkpoint number to resume training from\")\n",
    "\n",
    "    def parse_args(self, args_list=None):\n",
    "        \"\"\"\n",
    "        Parse and return the arguments.\n",
    "        Args:\n",
    "            args_list (list): List of arguments to parse (for notebook usage).\n",
    "        \"\"\"\n",
    "        if args_list is None:\n",
    "            # Parse from command line\n",
    "            return self.parser.parse_args()\n",
    "        else:\n",
    "            # Parse from a list (for notebook usage)\n",
    "            return self.parser.parse_args(args_list)\n",
    "\n",
    "# Usage in Jupyter Notebook\n",
    "if __name__ == \"__main__\":\n",
    "    config = Config()\n",
    "    args = config.parse_args()\n",
    "    print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
