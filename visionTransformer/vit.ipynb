{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\.conda\\envs\\mmdit\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Trainer in module transformers.trainer:\n",
      "\n",
      "class Trainer(builtins.object)\n",
      " |  Trainer(model: Union[transformers.modeling_utils.PreTrainedModel, torch.nn.modules.module.Module] = None, args: transformers.training_args.TrainingArguments = None, data_collator: Optional[transformers.data.data_collator.DataCollator] = None, train_dataset: Union[torch.utils.data.dataset.Dataset, torch.utils.data.dataset.IterableDataset, ForwardRef('datasets.Dataset'), NoneType] = None, eval_dataset: Union[torch.utils.data.dataset.Dataset, Dict[str, torch.utils.data.dataset.Dataset], ForwardRef('datasets.Dataset'), NoneType] = None, processing_class: Union[transformers.tokenization_utils_base.PreTrainedTokenizerBase, transformers.image_processing_utils.BaseImageProcessor, transformers.feature_extraction_utils.FeatureExtractionMixin, transformers.processing_utils.ProcessorMixin, NoneType] = None, model_init: Optional[Callable[[], transformers.modeling_utils.PreTrainedModel]] = None, compute_loss_func: Optional[Callable] = None, compute_metrics: Optional[Callable[[transformers.trainer_utils.EvalPrediction], Dict]] = None, callbacks: Optional[List[transformers.trainer_callback.TrainerCallback]] = None, optimizers: Tuple[Optional[torch.optim.optimizer.Optimizer], Optional[torch.optim.lr_scheduler.LambdaLR]] = (None, None), optimizer_cls_and_kwargs: Optional[Tuple[Type[torch.optim.optimizer.Optimizer], Dict[str, Any]]] = None, preprocess_logits_for_metrics: Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]] = None)\n",
      " |  \n",
      " |  Trainer is a simple but feature-complete training and eval loop for PyTorch, optimized for ðŸ¤— Transformers.\n",
      " |  \n",
      " |  Args:\n",
      " |      model ([`PreTrainedModel`] or `torch.nn.Module`, *optional*):\n",
      " |          The model to train, evaluate or use for predictions. If not provided, a `model_init` must be passed.\n",
      " |  \n",
      " |          <Tip>\n",
      " |  \n",
      " |          [`Trainer`] is optimized to work with the [`PreTrainedModel`] provided by the library. You can still use\n",
      " |          your own models defined as `torch.nn.Module` as long as they work the same way as the ðŸ¤— Transformers\n",
      " |          models.\n",
      " |  \n",
      " |          </Tip>\n",
      " |  \n",
      " |      args ([`TrainingArguments`], *optional*):\n",
      " |          The arguments to tweak for training. Will default to a basic instance of [`TrainingArguments`] with the\n",
      " |          `output_dir` set to a directory named *tmp_trainer* in the current directory if not provided.\n",
      " |      data_collator (`DataCollator`, *optional*):\n",
      " |          The function to use to form a batch from a list of elements of `train_dataset` or `eval_dataset`. Will\n",
      " |          default to [`default_data_collator`] if no `processing_class` is provided, an instance of\n",
      " |          [`DataCollatorWithPadding`] otherwise if the processing_class is a feature extractor or tokenizer.\n",
      " |      train_dataset (Union[`torch.utils.data.Dataset`, `torch.utils.data.IterableDataset`, `datasets.Dataset`], *optional*):\n",
      " |          The dataset to use for training. If it is a [`~datasets.Dataset`], columns not accepted by the\n",
      " |          `model.forward()` method are automatically removed.\n",
      " |  \n",
      " |          Note that if it's a `torch.utils.data.IterableDataset` with some randomization and you are training in a\n",
      " |          distributed fashion, your iterable dataset should either use a internal attribute `generator` that is a\n",
      " |          `torch.Generator` for the randomization that must be identical on all processes (and the Trainer will\n",
      " |          manually set the seed of this `generator` at each epoch) or have a `set_epoch()` method that internally\n",
      " |          sets the seed of the RNGs used.\n",
      " |      eval_dataset (Union[`torch.utils.data.Dataset`, Dict[str, `torch.utils.data.Dataset`, `datasets.Dataset`]), *optional*):\n",
      " |           The dataset to use for evaluation. If it is a [`~datasets.Dataset`], columns not accepted by the\n",
      " |           `model.forward()` method are automatically removed. If it is a dictionary, it will evaluate on each\n",
      " |           dataset prepending the dictionary key to the metric name.\n",
      " |      processing_class (`PreTrainedTokenizerBase` or `BaseImageProcessor` or `FeatureExtractionMixin` or `ProcessorMixin`, *optional*):\n",
      " |          Processing class used to process the data. If provided, will be used to automatically process the inputs\n",
      " |          for the model, and it will be saved along the model to make it easier to rerun an interrupted training or\n",
      " |          reuse the fine-tuned model.\n",
      " |          This supercedes the `tokenizer` argument, which is now deprecated.\n",
      " |      model_init (`Callable[[], PreTrainedModel]`, *optional*):\n",
      " |          A function that instantiates the model to be used. If provided, each call to [`~Trainer.train`] will start\n",
      " |          from a new instance of the model as given by this function.\n",
      " |  \n",
      " |          The function may have zero argument, or a single one containing the optuna/Ray Tune/SigOpt trial object, to\n",
      " |          be able to choose different architectures according to hyper parameters (such as layer count, sizes of\n",
      " |          inner layers, dropout probabilities etc).\n",
      " |      compute_loss_func (`Callable`, *optional*):\n",
      " |          A function that accepts the raw model outputs, labels, and the number of items in the entire accumulated\n",
      " |          batch (batch_size * gradient_accumulation_steps) and returns the loss. For example, see the default [loss function](https://github.com/huggingface/transformers/blob/052e652d6d53c2b26ffde87e039b723949a53493/src/transformers/trainer.py#L3618) used by [`Trainer`].\n",
      " |      compute_metrics (`Callable[[EvalPrediction], Dict]`, *optional*):\n",
      " |          The function that will be used to compute metrics at evaluation. Must take a [`EvalPrediction`] and return\n",
      " |          a dictionary string to metric values. *Note* When passing TrainingArgs with `batch_eval_metrics` set to\n",
      " |          `True`, your compute_metrics function must take a boolean `compute_result` argument. This will be triggered\n",
      " |          after the last eval batch to signal that the function needs to calculate and return the global summary\n",
      " |          statistics rather than accumulating the batch-level statistics\n",
      " |      callbacks (List of [`TrainerCallback`], *optional*):\n",
      " |          A list of callbacks to customize the training loop. Will add those to the list of default callbacks\n",
      " |          detailed in [here](callback).\n",
      " |  \n",
      " |          If you want to remove one of the default callbacks used, use the [`Trainer.remove_callback`] method.\n",
      " |      optimizers (`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`, *optional*, defaults to `(None, None)`):\n",
      " |          A tuple containing the optimizer and the scheduler to use. Will default to an instance of [`AdamW`] on your\n",
      " |          model and a scheduler given by [`get_linear_schedule_with_warmup`] controlled by `args`.\n",
      " |      optimizer_cls_and_kwargs (`Tuple[Type[torch.optim.Optimizer], Dict[str, Any]]`, *optional*):\n",
      " |          A tuple containing the optimizer class and keyword arguments to use.\n",
      " |          Overrides `optim` and `optim_args` in `args`. Incompatible with the `optimizers` argument.\n",
      " |  \n",
      " |          Unlike `optimizers`, this argument avoids the need to place model parameters on the correct devices before initializing the Trainer.\n",
      " |      preprocess_logits_for_metrics (`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`, *optional*):\n",
      " |          A function that preprocess the logits right before caching them at each evaluation step. Must take two\n",
      " |          tensors, the logits and the labels, and return the logits once processed as desired. The modifications made\n",
      " |          by this function will be reflected in the predictions received by `compute_metrics`.\n",
      " |  \n",
      " |          Note that the labels (second parameter) will be `None` if the dataset does not have them.\n",
      " |  \n",
      " |  Important attributes:\n",
      " |  \n",
      " |      - **model** -- Always points to the core model. If using a transformers model, it will be a [`PreTrainedModel`]\n",
      " |        subclass.\n",
      " |      - **model_wrapped** -- Always points to the most external model in case one or more other modules wrap the\n",
      " |        original model. This is the model that should be used for the forward pass. For example, under `DeepSpeed`,\n",
      " |        the inner model is wrapped in `DeepSpeed` and then again in `torch.nn.DistributedDataParallel`. If the inner\n",
      " |        model hasn't been wrapped, then `self.model_wrapped` is the same as `self.model`.\n",
      " |      - **is_model_parallel** -- Whether or not a model has been switched to a model parallel mode (different from\n",
      " |        data parallelism, this means some of the model layers are split on different GPUs).\n",
      " |      - **place_model_on_device** -- Whether or not to automatically place the model on the device - it will be set\n",
      " |        to `False` if model parallel or deepspeed is used, or if the default\n",
      " |        `TrainingArguments.place_model_on_device` is overridden to return `False` .\n",
      " |      - **is_in_train** -- Whether or not a model is currently running `train` (e.g. when `evaluate` is called while\n",
      " |        in `train`)\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, model: Union[transformers.modeling_utils.PreTrainedModel, torch.nn.modules.module.Module] = None, args: transformers.training_args.TrainingArguments = None, data_collator: Optional[transformers.data.data_collator.DataCollator] = None, train_dataset: Union[torch.utils.data.dataset.Dataset, torch.utils.data.dataset.IterableDataset, ForwardRef('datasets.Dataset'), NoneType] = None, eval_dataset: Union[torch.utils.data.dataset.Dataset, Dict[str, torch.utils.data.dataset.Dataset], ForwardRef('datasets.Dataset'), NoneType] = None, processing_class: Union[transformers.tokenization_utils_base.PreTrainedTokenizerBase, transformers.image_processing_utils.BaseImageProcessor, transformers.feature_extraction_utils.FeatureExtractionMixin, transformers.processing_utils.ProcessorMixin, NoneType] = None, model_init: Optional[Callable[[], transformers.modeling_utils.PreTrainedModel]] = None, compute_loss_func: Optional[Callable] = None, compute_metrics: Optional[Callable[[transformers.trainer_utils.EvalPrediction], Dict]] = None, callbacks: Optional[List[transformers.trainer_callback.TrainerCallback]] = None, optimizers: Tuple[Optional[torch.optim.optimizer.Optimizer], Optional[torch.optim.lr_scheduler.LambdaLR]] = (None, None), optimizer_cls_and_kwargs: Optional[Tuple[Type[torch.optim.optimizer.Optimizer], Dict[str, Any]]] = None, preprocess_logits_for_metrics: Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]] = None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  add_callback(self, callback)\n",
      " |      Add a callback to the current list of [`~transformers.TrainerCallback`].\n",
      " |      \n",
      " |      Args:\n",
      " |         callback (`type` or [`~transformers.TrainerCallback]`):\n",
      " |             A [`~transformers.TrainerCallback`] class or an instance of a [`~transformers.TrainerCallback`]. In the\n",
      " |             first case, will instantiate a member of that class.\n",
      " |  \n",
      " |  autocast_smart_context_manager(self, cache_enabled: Optional[bool] = True)\n",
      " |      A helper wrapper that creates an appropriate context manager for `autocast` while feeding it the desired\n",
      " |      arguments, depending on the situation.\n",
      " |  \n",
      " |  call_model_init(self, trial=None)\n",
      " |  \n",
      " |  compare_trainer_and_checkpoint_args(self, training_args, trainer_state)\n",
      " |  \n",
      " |  compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None)\n",
      " |      How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
      " |      \n",
      " |      Subclass and override for custom behavior.\n",
      " |  \n",
      " |  compute_loss_context_manager(self)\n",
      " |      A helper wrapper to group together context managers.\n",
      " |  \n",
      " |  create_accelerator_and_postprocess(self)\n",
      " |  \n",
      " |  create_model_card(self, language: Optional[str] = None, license: Optional[str] = None, tags: Union[str, List[str], NoneType] = None, model_name: Optional[str] = None, finetuned_from: Optional[str] = None, tasks: Union[str, List[str], NoneType] = None, dataset_tags: Union[str, List[str], NoneType] = None, dataset: Union[str, List[str], NoneType] = None, dataset_args: Union[str, List[str], NoneType] = None)\n",
      " |      Creates a draft of a model card using the information available to the `Trainer`.\n",
      " |      \n",
      " |      Args:\n",
      " |          language (`str`, *optional*):\n",
      " |              The language of the model (if applicable)\n",
      " |          license (`str`, *optional*):\n",
      " |              The license of the model. Will default to the license of the pretrained model used, if the original\n",
      " |              model given to the `Trainer` comes from a repo on the Hub.\n",
      " |          tags (`str` or `List[str]`, *optional*):\n",
      " |              Some tags to be included in the metadata of the model card.\n",
      " |          model_name (`str`, *optional*):\n",
      " |              The name of the model.\n",
      " |          finetuned_from (`str`, *optional*):\n",
      " |              The name of the model used to fine-tune this one (if applicable). Will default to the name of the repo\n",
      " |              of the original model given to the `Trainer` (if it comes from the Hub).\n",
      " |          tasks (`str` or `List[str]`, *optional*):\n",
      " |              One or several task identifiers, to be included in the metadata of the model card.\n",
      " |          dataset_tags (`str` or `List[str]`, *optional*):\n",
      " |              One or several dataset tags, to be included in the metadata of the model card.\n",
      " |          dataset (`str` or `List[str]`, *optional*):\n",
      " |              One or several dataset identifiers, to be included in the metadata of the model card.\n",
      " |          dataset_args (`str` or `List[str]`, *optional*):\n",
      " |             One or several dataset arguments, to be included in the metadata of the model card.\n",
      " |  \n",
      " |  create_optimizer(self)\n",
      " |      Setup the optimizer.\n",
      " |      \n",
      " |      We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\n",
      " |      Trainer's init through `optimizers`, or subclass and override this method in a subclass.\n",
      " |  \n",
      " |  create_optimizer_and_scheduler(self, num_training_steps: int)\n",
      " |      Setup the optimizer and the learning rate scheduler.\n",
      " |      \n",
      " |      We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\n",
      " |      Trainer's init through `optimizers`, or subclass and override this method (or `create_optimizer` and/or\n",
      " |      `create_scheduler`) in a subclass.\n",
      " |  \n",
      " |  create_scheduler(self, num_training_steps: int, optimizer: torch.optim.optimizer.Optimizer = None)\n",
      " |      Setup the scheduler. The optimizer of the trainer must have been set up either before this method is called or\n",
      " |      passed as an argument.\n",
      " |      \n",
      " |      Args:\n",
      " |          num_training_steps (int): The number of training steps to do.\n",
      " |  \n",
      " |  evaluate(self, eval_dataset: Union[torch.utils.data.dataset.Dataset, Dict[str, torch.utils.data.dataset.Dataset], NoneType] = None, ignore_keys: Optional[List[str]] = None, metric_key_prefix: str = 'eval') -> Dict[str, float]\n",
      " |      Run evaluation and returns metrics.\n",
      " |      \n",
      " |      The calling script will be responsible for providing a method to compute metrics, as they are task-dependent\n",
      " |      (pass it to the init `compute_metrics` argument).\n",
      " |      \n",
      " |      You can also subclass and override this method to inject custom behavior.\n",
      " |      \n",
      " |      Args:\n",
      " |          eval_dataset (Union[`Dataset`, Dict[str, `Dataset`]), *optional*):\n",
      " |              Pass a dataset if you wish to override `self.eval_dataset`. If it is a [`~datasets.Dataset`], columns\n",
      " |              not accepted by the `model.forward()` method are automatically removed. If it is a dictionary, it will\n",
      " |              evaluate on each dataset, prepending the dictionary key to the metric name. Datasets must implement the\n",
      " |              `__len__` method.\n",
      " |      \n",
      " |              <Tip>\n",
      " |      \n",
      " |              If you pass a dictionary with names of datasets as keys and datasets as values, evaluate will run\n",
      " |              separate evaluations on each dataset. This can be useful to monitor how training affects other\n",
      " |              datasets or simply to get a more fine-grained evaluation.\n",
      " |              When used with `load_best_model_at_end`, make sure `metric_for_best_model` references exactly one\n",
      " |              of the datasets. If you, for example, pass in `{\"data1\": data1, \"data2\": data2}` for two datasets\n",
      " |              `data1` and `data2`, you could specify `metric_for_best_model=\"eval_data1_loss\"` for using the\n",
      " |              loss on `data1` and `metric_for_best_model=\"eval_data2_loss\"` for the loss on `data2`.\n",
      " |      \n",
      " |              </Tip>\n",
      " |      \n",
      " |          ignore_keys (`List[str]`, *optional*):\n",
      " |              A list of keys in the output of your model (if it is a dictionary) that should be ignored when\n",
      " |              gathering predictions.\n",
      " |          metric_key_prefix (`str`, *optional*, defaults to `\"eval\"`):\n",
      " |              An optional prefix to be used as the metrics key prefix. For example the metrics \"bleu\" will be named\n",
      " |              \"eval_bleu\" if the prefix is \"eval\" (default)\n",
      " |      \n",
      " |      Returns:\n",
      " |          A dictionary containing the evaluation loss and the potential metrics computed from the predictions. The\n",
      " |          dictionary also contains the epoch number which comes from the training state.\n",
      " |  \n",
      " |  evaluation_loop(self, dataloader: torch.utils.data.dataloader.DataLoader, description: str, prediction_loss_only: Optional[bool] = None, ignore_keys: Optional[List[str]] = None, metric_key_prefix: str = 'eval') -> transformers.trainer_utils.EvalLoopOutput\n",
      " |      Prediction/evaluation loop, shared by `Trainer.evaluate()` and `Trainer.predict()`.\n",
      " |      \n",
      " |      Works both with or without labels.\n",
      " |  \n",
      " |  floating_point_ops(self, inputs: Dict[str, Union[torch.Tensor, Any]])\n",
      " |      For models that inherit from [`PreTrainedModel`], uses that method to compute the number of floating point\n",
      " |      operations for every backward + forward pass. If using another model, either implement such a method in the\n",
      " |      model or subclass and override this method.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n",
      " |              The inputs and targets of the model.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `int`: The number of floating-point operations.\n",
      " |  \n",
      " |  get_batch_samples(self, epoch_iterator, num_batches)\n",
      " |  \n",
      " |  get_decay_parameter_names(self, model) -> List[str]\n",
      " |      Get all parameter names that weight decay will be applied to\n",
      " |      \n",
      " |      Note that some models implement their own layernorm instead of calling nn.LayerNorm, weight decay could still\n",
      " |      apply to those modules since this function only filter out instance of nn.LayerNorm\n",
      " |  \n",
      " |  get_eval_dataloader(self, eval_dataset: Union[str, torch.utils.data.dataset.Dataset, NoneType] = None) -> torch.utils.data.dataloader.DataLoader\n",
      " |      Returns the evaluation [`~torch.utils.data.DataLoader`].\n",
      " |      \n",
      " |      Subclass and override this method if you want to inject some custom behavior.\n",
      " |      \n",
      " |      Args:\n",
      " |          eval_dataset (`str` or `torch.utils.data.Dataset`, *optional*):\n",
      " |              If a `str`, will use `self.eval_dataset[eval_dataset]` as the evaluation dataset. If a `Dataset`, will override `self.eval_dataset` and must implement `__len__`. If it is a [`~datasets.Dataset`], columns not accepted by the `model.forward()` method are automatically removed.\n",
      " |  \n",
      " |  get_learning_rates(self)\n",
      " |      Returns the learning rate of each parameter from self.optimizer.\n",
      " |  \n",
      " |  get_num_trainable_parameters(self)\n",
      " |      Get the number of trainable parameters.\n",
      " |  \n",
      " |  get_optimizer_group(self, param: Union[str, torch.nn.parameter.Parameter, NoneType] = None)\n",
      " |      Returns optimizer group for a parameter if given, else returns all optimizer groups for params.\n",
      " |      \n",
      " |      Args:\n",
      " |          param (`str` or `torch.nn.parameter.Parameter`, *optional*):\n",
      " |              The parameter for which optimizer group needs to be returned.\n",
      " |  \n",
      " |  get_test_dataloader(self, test_dataset: torch.utils.data.dataset.Dataset) -> torch.utils.data.dataloader.DataLoader\n",
      " |      Returns the test [`~torch.utils.data.DataLoader`].\n",
      " |      \n",
      " |      Subclass and override this method if you want to inject some custom behavior.\n",
      " |      \n",
      " |      Args:\n",
      " |          test_dataset (`torch.utils.data.Dataset`, *optional*):\n",
      " |              The test dataset to use. If it is a [`~datasets.Dataset`], columns not accepted by the\n",
      " |              `model.forward()` method are automatically removed. It must implement `__len__`.\n",
      " |  \n",
      " |  get_train_dataloader(self) -> torch.utils.data.dataloader.DataLoader\n",
      " |      Returns the training [`~torch.utils.data.DataLoader`].\n",
      " |      \n",
      " |      Will use no sampler if `train_dataset` does not implement `__len__`, a random sampler (adapted to distributed\n",
      " |      training if necessary) otherwise.\n",
      " |      \n",
      " |      Subclass and override this method if you want to inject some custom behavior.\n",
      " |  \n",
      " |  hyperparameter_search(self, hp_space: Optional[Callable[[ForwardRef('optuna.Trial')], Dict[str, float]]] = None, compute_objective: Optional[Callable[[Dict[str, float]], float]] = None, n_trials: int = 20, direction: Union[str, List[str]] = 'minimize', backend: Union[ForwardRef('str'), transformers.trainer_utils.HPSearchBackend, NoneType] = None, hp_name: Optional[Callable[[ForwardRef('optuna.Trial')], str]] = None, **kwargs) -> Union[transformers.trainer_utils.BestRun, List[transformers.trainer_utils.BestRun]]\n",
      " |      Launch an hyperparameter search using `optuna` or `Ray Tune` or `SigOpt`. The optimized quantity is determined\n",
      " |      by `compute_objective`, which defaults to a function returning the evaluation loss when no metric is provided,\n",
      " |      the sum of all metrics otherwise.\n",
      " |      \n",
      " |      <Tip warning={true}>\n",
      " |      \n",
      " |      To use this method, you need to have provided a `model_init` when initializing your [`Trainer`]: we need to\n",
      " |      reinitialize the model at each new run. This is incompatible with the `optimizers` argument, so you need to\n",
      " |      subclass [`Trainer`] and override the method [`~Trainer.create_optimizer_and_scheduler`] for custom\n",
      " |      optimizer/scheduler.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Args:\n",
      " |          hp_space (`Callable[[\"optuna.Trial\"], Dict[str, float]]`, *optional*):\n",
      " |              A function that defines the hyperparameter search space. Will default to\n",
      " |              [`~trainer_utils.default_hp_space_optuna`] or [`~trainer_utils.default_hp_space_ray`] or\n",
      " |              [`~trainer_utils.default_hp_space_sigopt`] depending on your backend.\n",
      " |          compute_objective (`Callable[[Dict[str, float]], float]`, *optional*):\n",
      " |              A function computing the objective to minimize or maximize from the metrics returned by the `evaluate`\n",
      " |              method. Will default to [`~trainer_utils.default_compute_objective`].\n",
      " |          n_trials (`int`, *optional*, defaults to 100):\n",
      " |              The number of trial runs to test.\n",
      " |          direction (`str` or `List[str]`, *optional*, defaults to `\"minimize\"`):\n",
      " |              If it's single objective optimization, direction is `str`, can be `\"minimize\"` or `\"maximize\"`, you\n",
      " |              should pick `\"minimize\"` when optimizing the validation loss, `\"maximize\"` when optimizing one or\n",
      " |              several metrics. If it's multi objectives optimization, direction is `List[str]`, can be List of\n",
      " |              `\"minimize\"` and `\"maximize\"`, you should pick `\"minimize\"` when optimizing the validation loss,\n",
      " |              `\"maximize\"` when optimizing one or several metrics.\n",
      " |          backend (`str` or [`~training_utils.HPSearchBackend`], *optional*):\n",
      " |              The backend to use for hyperparameter search. Will default to optuna or Ray Tune or SigOpt, depending\n",
      " |              on which one is installed. If all are installed, will default to optuna.\n",
      " |          hp_name (`Callable[[\"optuna.Trial\"], str]]`, *optional*):\n",
      " |              A function that defines the trial/run name. Will default to None.\n",
      " |          kwargs (`Dict[str, Any]`, *optional*):\n",
      " |              Additional keyword arguments for each backend:\n",
      " |      \n",
      " |              - `optuna`: parameters from\n",
      " |                [optuna.study.create_study](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.create_study.html)\n",
      " |                and also the parameters `timeout`, `n_jobs` and `gc_after_trial` from\n",
      " |                [optuna.study.Study.optimize](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.Study.html#optuna.study.Study.optimize)\n",
      " |              - `ray`: parameters from [tune.run](https://docs.ray.io/en/latest/tune/api_docs/execution.html#tune-run).\n",
      " |                If `resources_per_trial` is not set in the `kwargs`, it defaults to 1 CPU core and 1 GPU (if available).\n",
      " |                If `progress_reporter` is not set in the `kwargs`,\n",
      " |                [ray.tune.CLIReporter](https://docs.ray.io/en/latest/tune/api/doc/ray.tune.CLIReporter.html) is used.\n",
      " |              - `sigopt`: the parameter `proxies` from\n",
      " |                [sigopt.Connection.set_proxies](https://docs.sigopt.com/support/faq#how-do-i-use-sigopt-with-a-proxy).\n",
      " |      \n",
      " |      Returns:\n",
      " |          [`trainer_utils.BestRun` or `List[trainer_utils.BestRun]`]: All the information about the best run or best\n",
      " |          runs for multi-objective optimization. Experiment summary can be found in `run_summary` attribute for Ray\n",
      " |          backend.\n",
      " |  \n",
      " |  init_hf_repo(self, token: Optional[str] = None)\n",
      " |      Initializes a git repo in `self.args.hub_model_id`.\n",
      " |  \n",
      " |  ipex_optimize_model(self, model, training=False, dtype=torch.float32)\n",
      " |  \n",
      " |  is_local_process_zero(self) -> bool\n",
      " |      Whether or not this process is the local (e.g., on one machine if training in a distributed fashion on several\n",
      " |      machines) main process.\n",
      " |  \n",
      " |  is_world_process_zero(self) -> bool\n",
      " |      Whether or not this process is the global main process (when training in a distributed fashion on several\n",
      " |      machines, this is only going to be `True` for one process).\n",
      " |  \n",
      " |  log(self, logs: Dict[str, float], start_time: Optional[float] = None) -> None\n",
      " |      Log `logs` on the various objects watching training.\n",
      " |      \n",
      " |      Subclass and override this method to inject custom behavior.\n",
      " |      \n",
      " |      Args:\n",
      " |          logs (`Dict[str, float]`):\n",
      " |              The values to log.\n",
      " |          start_time (`Optional[float]`):\n",
      " |              The start of training.\n",
      " |  \n",
      " |  log_metrics(self, split, metrics)\n",
      " |      Log metrics in a specially formatted way\n",
      " |      \n",
      " |      Under distributed environment this is done only for a process with rank 0.\n",
      " |      \n",
      " |      Args:\n",
      " |          split (`str`):\n",
      " |              Mode/split name: one of `train`, `eval`, `test`\n",
      " |          metrics (`Dict[str, float]`):\n",
      " |              The metrics returned from train/evaluate/predictmetrics: metrics dict\n",
      " |      \n",
      " |      Notes on memory reports:\n",
      " |      \n",
      " |      In order to get memory usage report you need to install `psutil`. You can do that with `pip install psutil`.\n",
      " |      \n",
      " |      Now when this method is run, you will see a report that will include: :\n",
      " |      \n",
      " |      ```\n",
      " |      init_mem_cpu_alloc_delta   =     1301MB\n",
      " |      init_mem_cpu_peaked_delta  =      154MB\n",
      " |      init_mem_gpu_alloc_delta   =      230MB\n",
      " |      init_mem_gpu_peaked_delta  =        0MB\n",
      " |      train_mem_cpu_alloc_delta  =     1345MB\n",
      " |      train_mem_cpu_peaked_delta =        0MB\n",
      " |      train_mem_gpu_alloc_delta  =      693MB\n",
      " |      train_mem_gpu_peaked_delta =        7MB\n",
      " |      ```\n",
      " |      \n",
      " |      **Understanding the reports:**\n",
      " |      \n",
      " |      - the first segment, e.g., `train__`, tells you which stage the metrics are for. Reports starting with `init_`\n",
      " |          will be added to the first stage that gets run. So that if only evaluation is run, the memory usage for the\n",
      " |          `__init__` will be reported along with the `eval_` metrics.\n",
      " |      - the third segment, is either `cpu` or `gpu`, tells you whether it's the general RAM or the gpu0 memory\n",
      " |          metric.\n",
      " |      - `*_alloc_delta` - is the difference in the used/allocated memory counter between the end and the start of the\n",
      " |          stage - it can be negative if a function released more memory than it allocated.\n",
      " |      - `*_peaked_delta` - is any extra memory that was consumed and then freed - relative to the current allocated\n",
      " |          memory counter - it is never negative. When you look at the metrics of any stage you add up `alloc_delta` +\n",
      " |          `peaked_delta` and you know how much memory was needed to complete that stage.\n",
      " |      \n",
      " |      The reporting happens only for process of rank 0 and gpu 0 (if there is a gpu). Typically this is enough since the\n",
      " |      main process does the bulk of work, but it could be not quite so if model parallel is used and then other GPUs may\n",
      " |      use a different amount of gpu memory. This is also not the same under DataParallel where gpu0 may require much more\n",
      " |      memory than the rest since it stores the gradient and optimizer states for all participating GPUS. Perhaps in the\n",
      " |      future these reports will evolve to measure those too.\n",
      " |      \n",
      " |      The CPU RAM metric measures RSS (Resident Set Size) includes both the memory which is unique to the process and the\n",
      " |      memory shared with other processes. It is important to note that it does not include swapped out memory, so the\n",
      " |      reports could be imprecise.\n",
      " |      \n",
      " |      The CPU peak memory is measured using a sampling thread. Due to python's GIL it may miss some of the peak memory if\n",
      " |      that thread didn't get a chance to run when the highest memory was used. Therefore this report can be less than\n",
      " |      reality. Using `tracemalloc` would have reported the exact peak memory, but it doesn't report memory allocations\n",
      " |      outside of python. So if some C++ CUDA extension allocated its own memory it won't be reported. And therefore it\n",
      " |      was dropped in favor of the memory sampling approach, which reads the current process memory usage.\n",
      " |      \n",
      " |      The GPU allocated and peak memory reporting is done with `torch.cuda.memory_allocated()` and\n",
      " |      `torch.cuda.max_memory_allocated()`. This metric reports only \"deltas\" for pytorch-specific allocations, as\n",
      " |      `torch.cuda` memory management system doesn't track any memory allocated outside of pytorch. For example, the very\n",
      " |      first cuda call typically loads CUDA kernels, which may take from 0.5 to 2GB of GPU memory.\n",
      " |      \n",
      " |      Note that this tracker doesn't account for memory allocations outside of [`Trainer`]'s `__init__`, `train`,\n",
      " |      `evaluate` and `predict` calls.\n",
      " |      \n",
      " |      Because `evaluation` calls may happen during `train`, we can't handle nested invocations because\n",
      " |      `torch.cuda.max_memory_allocated` is a single counter, so if it gets reset by a nested eval call, `train`'s tracker\n",
      " |      will report incorrect info. If this [pytorch issue](https://github.com/pytorch/pytorch/issues/16266) gets resolved\n",
      " |      it will be possible to change this class to be re-entrant. Until then we will only track the outer level of\n",
      " |      `train`, `evaluate` and `predict` methods. Which means that if `eval` is called during `train`, it's the latter\n",
      " |      that will account for its memory usage and that of the former.\n",
      " |      \n",
      " |      This also means that if any other tool that is used along the [`Trainer`] calls\n",
      " |      `torch.cuda.reset_peak_memory_stats`, the gpu peak memory stats could be invalid. And the [`Trainer`] will disrupt\n",
      " |      the normal behavior of any such tools that rely on calling `torch.cuda.reset_peak_memory_stats` themselves.\n",
      " |      \n",
      " |      For best performance you may want to consider turning the memory profiling off for production runs.\n",
      " |  \n",
      " |  metrics_format(self, metrics: Dict[str, float]) -> Dict[str, float]\n",
      " |      Reformat Trainer metrics values to a human-readable format\n",
      " |      \n",
      " |      Args:\n",
      " |          metrics (`Dict[str, float]`):\n",
      " |              The metrics returned from train/evaluate/predict\n",
      " |      \n",
      " |      Returns:\n",
      " |          metrics (`Dict[str, float]`): The reformatted metrics\n",
      " |  \n",
      " |  num_examples(self, dataloader: torch.utils.data.dataloader.DataLoader) -> int\n",
      " |      Helper to get number of samples in a [`~torch.utils.data.DataLoader`] by accessing its dataset. When\n",
      " |      dataloader.dataset does not exist or has no length, estimates as best it can\n",
      " |  \n",
      " |  pop_callback(self, callback)\n",
      " |      Remove a callback from the current list of [`~transformers.TrainerCallback`] and returns it.\n",
      " |      \n",
      " |      If the callback is not found, returns `None` (and no error is raised).\n",
      " |      \n",
      " |      Args:\n",
      " |         callback (`type` or [`~transformers.TrainerCallback]`):\n",
      " |             A [`~transformers.TrainerCallback`] class or an instance of a [`~transformers.TrainerCallback`]. In the\n",
      " |             first case, will pop the first member of that class found in the list of callbacks.\n",
      " |      \n",
      " |      Returns:\n",
      " |          [`~transformers.TrainerCallback`]: The callback removed, if found.\n",
      " |  \n",
      " |  predict(self, test_dataset: torch.utils.data.dataset.Dataset, ignore_keys: Optional[List[str]] = None, metric_key_prefix: str = 'test') -> transformers.trainer_utils.PredictionOutput\n",
      " |      Run prediction and returns predictions and potential metrics.\n",
      " |      \n",
      " |      Depending on the dataset and your use case, your test dataset may contain labels. In that case, this method\n",
      " |      will also return metrics, like in `evaluate()`.\n",
      " |      \n",
      " |      Args:\n",
      " |          test_dataset (`Dataset`):\n",
      " |              Dataset to run the predictions on. If it is an `datasets.Dataset`, columns not accepted by the\n",
      " |              `model.forward()` method are automatically removed. Has to implement the method `__len__`\n",
      " |          ignore_keys (`List[str]`, *optional*):\n",
      " |              A list of keys in the output of your model (if it is a dictionary) that should be ignored when\n",
      " |              gathering predictions.\n",
      " |          metric_key_prefix (`str`, *optional*, defaults to `\"test\"`):\n",
      " |              An optional prefix to be used as the metrics key prefix. For example the metrics \"bleu\" will be named\n",
      " |              \"test_bleu\" if the prefix is \"test\" (default)\n",
      " |      \n",
      " |      <Tip>\n",
      " |      \n",
      " |      If your predictions or labels have different sequence length (for instance because you're doing dynamic padding\n",
      " |      in a token classification task) the predictions will be padded (on the right) to allow for concatenation into\n",
      " |      one array. The padding index is -100.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Returns: *NamedTuple* A namedtuple with the following keys:\n",
      " |      \n",
      " |          - predictions (`np.ndarray`): The predictions on `test_dataset`.\n",
      " |          - label_ids (`np.ndarray`, *optional*): The labels (if the dataset contained some).\n",
      " |          - metrics (`Dict[str, float]`, *optional*): The potential dictionary of metrics (if the dataset contained\n",
      " |            labels).\n",
      " |  \n",
      " |  prediction_loop(self, dataloader: torch.utils.data.dataloader.DataLoader, description: str, prediction_loss_only: Optional[bool] = None, ignore_keys: Optional[List[str]] = None, metric_key_prefix: str = 'eval') -> transformers.trainer_utils.EvalLoopOutput\n",
      " |      Prediction/evaluation loop, shared by `Trainer.evaluate()` and `Trainer.predict()`.\n",
      " |      \n",
      " |      Works both with or without labels.\n",
      " |  \n",
      " |  prediction_step(self, model: torch.nn.modules.module.Module, inputs: Dict[str, Union[torch.Tensor, Any]], prediction_loss_only: bool, ignore_keys: Optional[List[str]] = None) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]\n",
      " |      Perform an evaluation step on `model` using `inputs`.\n",
      " |      \n",
      " |      Subclass and override to inject custom behavior.\n",
      " |      \n",
      " |      Args:\n",
      " |          model (`nn.Module`):\n",
      " |              The model to evaluate.\n",
      " |          inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n",
      " |              The inputs and targets of the model.\n",
      " |      \n",
      " |              The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n",
      " |              argument `labels`. Check your model's documentation for all accepted arguments.\n",
      " |          prediction_loss_only (`bool`):\n",
      " |              Whether or not to return the loss only.\n",
      " |          ignore_keys (`List[str]`, *optional*):\n",
      " |              A list of keys in the output of your model (if it is a dictionary) that should be ignored when\n",
      " |              gathering predictions.\n",
      " |      \n",
      " |      Return:\n",
      " |          Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]: A tuple with the loss,\n",
      " |          logits and labels (each being optional).\n",
      " |  \n",
      " |  propagate_args_to_deepspeed(self, auto_find_batch_size=False)\n",
      " |      Sets values in the deepspeed plugin based on the Trainer args\n",
      " |  \n",
      " |  push_to_hub(self, commit_message: Optional[str] = 'End of training', blocking: bool = True, token: Optional[str] = None, revision: Optional[str] = None, **kwargs) -> str\n",
      " |      Upload `self.model` and `self.processing_class` to the ðŸ¤— model hub on the repo `self.args.hub_model_id`.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          commit_message (`str`, *optional*, defaults to `\"End of training\"`):\n",
      " |              Message to commit while pushing.\n",
      " |          blocking (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether the function should return only when the `git push` has finished.\n",
      " |          token (`str`, *optional*, defaults to `None`):\n",
      " |              Token with write permission to overwrite Trainer's original args.\n",
      " |          revision (`str`, *optional*):\n",
      " |              The git revision to commit from. Defaults to the head of the \"main\" branch.\n",
      " |          kwargs (`Dict[str, Any]`, *optional*):\n",
      " |              Additional keyword arguments passed along to [`~Trainer.create_model_card`].\n",
      " |      \n",
      " |      Returns:\n",
      " |          The URL of the repository where the model was pushed if `blocking=False`, or a `Future` object tracking the\n",
      " |          progress of the commit if `blocking=True`.\n",
      " |  \n",
      " |  remove_callback(self, callback)\n",
      " |      Remove a callback from the current list of [`~transformers.TrainerCallback`].\n",
      " |      \n",
      " |      Args:\n",
      " |         callback (`type` or [`~transformers.TrainerCallback]`):\n",
      " |             A [`~transformers.TrainerCallback`] class or an instance of a [`~transformers.TrainerCallback`]. In the\n",
      " |             first case, will remove the first member of that class found in the list of callbacks.\n",
      " |  \n",
      " |  save_metrics(self, split, metrics, combined=True)\n",
      " |      Save metrics into a json file for that split, e.g. `train_results.json`.\n",
      " |      \n",
      " |      Under distributed environment this is done only for a process with rank 0.\n",
      " |      \n",
      " |      Args:\n",
      " |          split (`str`):\n",
      " |              Mode/split name: one of `train`, `eval`, `test`, `all`\n",
      " |          metrics (`Dict[str, float]`):\n",
      " |              The metrics returned from train/evaluate/predict\n",
      " |          combined (`bool`, *optional*, defaults to `True`):\n",
      " |              Creates combined metrics by updating `all_results.json` with metrics of this call\n",
      " |      \n",
      " |      To understand the metrics please read the docstring of [`~Trainer.log_metrics`]. The only difference is that raw\n",
      " |      unformatted numbers are saved in the current method.\n",
      " |  \n",
      " |  save_model(self, output_dir: Optional[str] = None, _internal_call: bool = False)\n",
      " |      Will save the model, so you can reload it using `from_pretrained()`.\n",
      " |      \n",
      " |      Will only save from the main process.\n",
      " |  \n",
      " |  save_state(self)\n",
      " |      Saves the Trainer state, since Trainer.save_model saves only the tokenizer with the model\n",
      " |      \n",
      " |      Under distributed environment this is done only for a process with rank 0.\n",
      " |  \n",
      " |  store_flos(self)\n",
      " |  \n",
      " |  torch_jit_model_eval(self, model, dataloader, training=False)\n",
      " |  \n",
      " |  train(self, resume_from_checkpoint: Union[str, bool, NoneType] = None, trial: Union[ForwardRef('optuna.Trial'), Dict[str, Any]] = None, ignore_keys_for_eval: Optional[List[str]] = None, **kwargs)\n",
      " |      Main training entry point.\n",
      " |      \n",
      " |      Args:\n",
      " |          resume_from_checkpoint (`str` or `bool`, *optional*):\n",
      " |              If a `str`, local path to a saved checkpoint as saved by a previous instance of [`Trainer`]. If a\n",
      " |              `bool` and equals `True`, load the last checkpoint in *args.output_dir* as saved by a previous instance\n",
      " |              of [`Trainer`]. If present, training will resume from the model/optimizer/scheduler states loaded here.\n",
      " |          trial (`optuna.Trial` or `Dict[str, Any]`, *optional*):\n",
      " |              The trial run or the hyperparameter dictionary for hyperparameter search.\n",
      " |          ignore_keys_for_eval (`List[str]`, *optional*)\n",
      " |              A list of keys in the output of your model (if it is a dictionary) that should be ignored when\n",
      " |              gathering predictions for evaluation during the training.\n",
      " |          kwargs (`Dict[str, Any]`, *optional*):\n",
      " |              Additional keyword arguments used to hide deprecated arguments\n",
      " |  \n",
      " |  training_step(self, model: torch.nn.modules.module.Module, inputs: Dict[str, Union[torch.Tensor, Any]], num_items_in_batch=None) -> torch.Tensor\n",
      " |      Perform a training step on a batch of inputs.\n",
      " |      \n",
      " |      Subclass and override to inject custom behavior.\n",
      " |      \n",
      " |      Args:\n",
      " |          model (`nn.Module`):\n",
      " |              The model to train.\n",
      " |          inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n",
      " |              The inputs and targets of the model.\n",
      " |      \n",
      " |              The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n",
      " |              argument `labels`. Check your model's documentation for all accepted arguments.\n",
      " |      \n",
      " |      Return:\n",
      " |          `torch.Tensor`: The tensor with training loss on this batch.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  get_optimizer_cls_and_kwargs(args: transformers.training_args.TrainingArguments, model: Optional[transformers.modeling_utils.PreTrainedModel] = None) -> Tuple[Any, Any]\n",
      " |      Returns the optimizer class and optimizer parameters based on the training arguments.\n",
      " |      \n",
      " |      Args:\n",
      " |          args (`transformers.training_args.TrainingArguments`):\n",
      " |              The training arguments for the training session.\n",
      " |  \n",
      " |  num_tokens(train_dl: torch.utils.data.dataloader.DataLoader, max_steps: Optional[int] = None) -> int\n",
      " |      Helper to get number of tokens in a [`~torch.utils.data.DataLoader`] by enumerating dataloader.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  tokenizer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\.conda\\envs\\mmdit\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTImageProcessor\n",
    "\n",
    "processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "image_mean = processor.image_mean\n",
    "image_std = processor.image_std\n",
    "size = processor.size[\"height\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# getting the name of the directory\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# where the this file is present.\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m current \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(\u001b[38;5;18;43m__file__\u001b[39;49m))\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Getting the parent directory name\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# where the current directory is present.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m parent \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(current)\n",
      "\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# getting the name of the directory\n",
    "# where the this file is present.\n",
    "current = os.path.dirname(os.path.realpath(__file__))\n",
    "\n",
    "# Getting the parent directory name\n",
    "# where the current directory is present.\n",
    "parent = os.path.dirname(current)\n",
    "\n",
    "# adding the parent directory to \n",
    "# the sys.path.\n",
    "sys.path.append(parent)\n",
    "\n",
    "# now we can import the module in the parent\n",
    "# directory.\n",
    "print(current)\n",
    "print(parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4088626204.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[5], line 4\u001b[1;36m\u001b[0m\n\u001b[1;33m    from MMDit-Main-Project.rerunTraining import ImageCaptionDataset\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../MMDiT-Main-Project')\n",
    "\n",
    "from MMDit-Main-Project.rerunTraining import ImageCaptionDataset\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "\n",
    "def setup_dataloader(csv_location,root_dir,img_size,batch_size,num_workers):\n",
    "    \"\"\"Setup dataset and dataloader with optimized transforms\"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((img_size,img_size)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], inplace=True),\n",
    "    ])\n",
    "\n",
    "    dataset = ImageCaptionDataset(\n",
    "        csv_path=\"dataset/Flickr/captions.csv\",\n",
    "        root_dir=\"dataset/FLickr/images\",\n",
    "        transform=transform,\n",
    "        cache_size=1000  # Cache 1000 images in memory\n",
    "    )\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size= batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "        prefetch_factor=2,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from transformers import ViTForImageClassification, AdamW\n",
    "import torch.nn as nn\n",
    "\n",
    "class ViTLightningModule(pl.LightningModule):\n",
    "    def __init__(self, num_labels=10):\n",
    "        super(ViTLightningModule, self).__init__()\n",
    "        self.vit = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k',\n",
    "                                                              num_labels=10,\n",
    "                                                              id2label=id2label,\n",
    "                                                              label2id=label2id)\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        outputs = self.vit(pixel_values=pixel_values)\n",
    "        return outputs.logits\n",
    "        \n",
    "    def common_step(self, batch, batch_idx):\n",
    "        pixel_values = batch['pixel_values']\n",
    "        labels = batch['labels']\n",
    "        logits = self(pixel_values)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loss = criterion(logits, labels)\n",
    "        predictions = logits.argmax(-1)\n",
    "        correct = (predictions == labels).sum().item()\n",
    "        accuracy = correct/pixel_values.shape[0]\n",
    "\n",
    "        return loss, accuracy\n",
    "      \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, accuracy = self.common_step(batch, batch_idx)     \n",
    "        # logs metrics for each training_step,\n",
    "        # and the average across the epoch\n",
    "        self.log(\"training_loss\", loss)\n",
    "        self.log(\"training_accuracy\", accuracy)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, accuracy = self.common_step(batch, batch_idx)     \n",
    "        self.log(\"validation_loss\", loss, on_epoch=True)\n",
    "        self.log(\"validation_accuracy\", accuracy, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, accuracy = self.common_step(batch, batch_idx)     \n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # We could make the optimizer more fancy by adding a scheduler and specifying which parameters do\n",
    "        # not require weight_decay but just using AdamW out-of-the-box works fine\n",
    "        return AdamW(self.parameters(), lr=5e-5)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return train_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return val_dataloader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return test_dataloader\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
